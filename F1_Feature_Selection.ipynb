{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgyeUA40aslE"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Optzx97aahnJ"
   },
   "source": [
    "In this notebook, run through the different techniques in performing feature selection on the [Breast Cancer Dataset](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29). Most of the modules will come from [scikit-learn](https://scikit-learn.org/stable/), one of the most commonly used machine learning libraries. It features various machine learning algorithms and has built-in implementations of different feature selection methods. Using these, you will be able to compare which method works best for this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEnMK4DRNV1O"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZersTw6TH1Zj"
   },
   "outputs": [],
   "source": [
    "# for data processing and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn modules for feature selection and model evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, SelectKBest, SelectFromModel, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# libraries for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvMpn0VaazcC"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DspE2DYYPpRp"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('./data/breast_cancer_data.csv')\n",
    "\n",
    "# Print datatypes\n",
    "print(df.dtypes)\n",
    "\n",
    "# Describe columns\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkXWz0WnbEPd"
   },
   "source": [
    "## Remove Unwanted Features\n",
    " The column `Unnamed: 32` has `NaN` values for all rows. Moreover, the `id` is just an arbitrary number assigned to patients and has nothing to do with the diagnosis. Hence, you can remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73EVqL6_N2-T"
   },
   "outputs": [],
   "source": [
    "# Check if there are null values in any of the columns. You will see `Unnamed: 32` has a lot.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4l4BzTbZfTO"
   },
   "outputs": [],
   "source": [
    "# Remove Unnamed: 32 and id columns\n",
    "columns_to_remove = ['Unnamed: 32', 'id']\n",
    "df.drop(columns_to_remove, axis=1, inplace=True)\n",
    "\n",
    "# Check that the columns are indeed dropped\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJ9yk-r6bYdZ"
   },
   "source": [
    "## Integer Encode Diagnosis\n",
    "\n",
    " the target column, `diagnosis`, is encoded as a string type categorical variable: `M` for malignant and `B` for benign. You need to convert these into integers before training the model. Since there are only two classes, you can use `0` for benign and `1` for malignant. Let's create a column `diagnosis_int` containing this integer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPDxg0AO4g-N"
   },
   "outputs": [],
   "source": [
    "# Integer encode the target variable, diagnosis\n",
    "df[\"diagnosis_int\"] = (df[\"diagnosis\"] == 'M').astype('int')\n",
    "\n",
    "# Drop the previous string column\n",
    "df.drop(['diagnosis'], axis=1, inplace=True)\n",
    "\n",
    "# Check the new column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s08Owp2kb0SB"
   },
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTuyLttI5h0w"
   },
   "outputs": [],
   "source": [
    "# Split feature and target vectors\n",
    "X = df.drop(\"diagnosis_int\", 1)\n",
    "Y = df[\"diagnosis_int\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uULmswIiThX"
   },
   "source": [
    "### Fit the Model and Calculate Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JVl3UGpq7_I"
   },
   "outputs": [],
   "source": [
    "def fit_model(X, Y):\n",
    "    '''Use a RandomForestClassifier for this problem.'''\n",
    "    \n",
    "    # define the model to use\n",
    "    model = RandomForestClassifier(criterion='entropy', random_state=47)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg-QoSiErLgv"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(model, X_test_scaled, Y_test):\n",
    "    '''Get model evaluation metrics on the test set.'''\n",
    "    \n",
    "    # Get model predictions\n",
    "    y_predict_r = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate evaluation metrics for assesing performance of the model.\n",
    "    acc = accuracy_score(Y_test, y_predict_r)\n",
    "    roc = roc_auc_score(Y_test, y_predict_r)\n",
    "    prec = precision_score(Y_test, y_predict_r)\n",
    "    rec = recall_score(Y_test, y_predict_r)\n",
    "    f1 = f1_score(Y_test, y_predict_r)\n",
    "    \n",
    "    return acc, roc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F06PrANXrLrL"
   },
   "outputs": [],
   "source": [
    "def train_and_get_metrics(X, Y):\n",
    "    '''Train a Random Forest Classifier and get evaluation metrics'''\n",
    "    \n",
    "    # Split train and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
    "\n",
    "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Call the fit model function to train the model on the normalized features and the diagnosis values\n",
    "    model = fit_model(X_train_scaled, Y_train)\n",
    "\n",
    "    # Make predictions on test dataset and calculate metrics.\n",
    "    acc, roc, prec, rec, f1 = calculate_metrics(model, X_test_scaled, Y_test)\n",
    "\n",
    "    return acc, roc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdOOXiqSmH6p"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_features(X, Y):\n",
    "    '''Train model and display evaluation metrics.'''\n",
    "    \n",
    "    # Train the model, predict values and get metrics\n",
    "    acc, roc, prec, rec, f1 = train_and_get_metrics(X, Y)\n",
    "\n",
    "    # Construct a dataframe to display metrics.\n",
    "    display_df = pd.DataFrame([[acc, roc, prec, rec, f1, X.shape[1]]], columns=[\"Accuracy\", \"ROC\", \"Precision\", \"Recall\", \"F1 Score\", 'Feature Count'])\n",
    "    \n",
    "    return display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8A0pEIZiZka"
   },
   "source": [
    "Train the model with all features included then calculate the metrics. This will be your baseline and you will compare this to the next outputs when you do feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sXRVKV-nlwR"
   },
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "all_features_eval_df = evaluate_model_on_features(X, Y)\n",
    "all_features_eval_df.index = ['All features']\n",
    "\n",
    "# Initialize results dataframe\n",
    "results = all_features_eval_df\n",
    "\n",
    "# Check the metrics\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g--wIHmFBjgr"
   },
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8rBZqEfw45p"
   },
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# Calculate correlation matrix\n",
    "cor = df.corr() \n",
    "\n",
    "# Plot the correlation matrix\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.PuBu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZHg3wdKjDWX"
   },
   "source": [
    "## Filter Methods\n",
    "\n",
    "Use:\n",
    "\n",
    "* Pearson Correlation (numeric features - numeric target, *exception: when target is 0/1 coded*)\n",
    "* ANOVA f-test (numeric features - categorical target)\n",
    "* Chi-squared (categorical features - categorical target)\n",
    "\n",
    "Let's use some of these in the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with the target variable\n",
    "\n",
    "Let's start by determining which features are strongly correlated with the diagnosis (i.e. the target variable). Since we have numeric features and our target, although categorical, is 0/1 coded, we can use Pearson correlation to compute the scores for each feature. This is also categorized as *supervised* feature selection because we're taking into account the relationship of each feature with the target variable. Moreover, since only one variable's relationship to the target is taken at a time, this falls under *univariate feature selection*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m34lMYEy9MK"
   },
   "outputs": [],
   "source": [
    "# Get the absolute value of the correlation\n",
    "cor_target = abs(cor[\"diagnosis_int\"])\n",
    "\n",
    "# Select highly correlated features (thresold = 0.2)\n",
    "relevant_features = cor_target[cor_target>0.2]\n",
    "\n",
    "# Collect the names of the features\n",
    "names = [index for index, value in relevant_features.iteritems()]\n",
    "\n",
    "# Drop the target variable from the results\n",
    "names.remove('diagnosis_int')\n",
    "\n",
    "# Display the results\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBoG4iMfjKzp"
   },
   "source": [
    "Now try training the model again but only with the features in the columns you just gathered. You can observe that there is an improvement in the metrics compared to the model you trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiHBfYEc8Wqb"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model with new features\n",
    "strong_features_eval_df = evaluate_model_on_features(df[names], Y)\n",
    "strong_features_eval_df.index = ['Strong features']\n",
    "\n",
    "# Append to results and display\n",
    "results = results.append(strong_features_eval_df)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6q3QJIrje2S"
   },
   "source": [
    "### Correlation with other features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yIfyep00eQb"
   },
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# Calculate the correlation matrix for target relevant features that you previously determined\n",
    "new_corr = df[names].corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "sns.heatmap(new_corr, annot=True, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTXGrmiqkS-b"
   },
   "source": [
    "This is a more magnified view of the features that are highly correlated to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrzX3gpwwfKB"
   },
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "# Select a subset of features\n",
    "new_corr = df[['perimeter_mean', 'radius_worst', 'perimeter_worst', 'area_worst', 'radius_mean']].corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "sns.heatmap(new_corr, annot=True, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlDWwAZi8LPi"
   },
   "outputs": [],
   "source": [
    "# Remove the features with high correlation to other features\n",
    "subset_feature_corr_names = [x for x in names if x not in ['radius_worst', 'perimeter_worst', 'area_worst']]\n",
    "\n",
    "# Calculate and check evaluation metrics\n",
    "subset_feature_eval_df = evaluate_model_on_features(df[subset_feature_corr_names], Y)\n",
    "subset_feature_eval_df.index = ['Subset features']\n",
    "\n",
    "# Append to results and display\n",
    "results = results.append(subset_feature_eval_df)\n",
    "results.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVJ_tGofAiDv"
   },
   "source": [
    "### Univariate Selection with Sci-Kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcJLnTwEAg9U"
   },
   "outputs": [],
   "source": [
    "def univariate_selection():\n",
    "    \n",
    "    # Split train and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
    "    \n",
    "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # User SelectKBest to select top 20 features based on f-test\n",
    "    selector = SelectKBest(f_classif, k=20)\n",
    "    \n",
    "    # Fit to scaled data, then transform it\n",
    "    X_new = selector.fit_transform(X_train_scaled, Y_train)\n",
    "    \n",
    "    # Print the results\n",
    "    feature_idx = selector.get_support()\n",
    "    for name, included in zip(df.drop(\"diagnosis_int\",1 ).columns, feature_idx):\n",
    "        print(\"%s: %s\" % (name, included))\n",
    "    \n",
    "    # Drop the target variable\n",
    "    feature_names = df.drop(\"diagnosis_int\",1 ).columns[feature_idx]\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaeEb5f_HjQC"
   },
   "outputs": [],
   "source": [
    "univariate_feature_names = univariate_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTF7GQuvAzIk"
   },
   "outputs": [],
   "source": [
    "# Calculate and check model metrics\n",
    "univariate_eval_df = evaluate_model_on_features(df[univariate_feature_names], Y)\n",
    "univariate_eval_df.index = ['F-test']\n",
    "\n",
    "# Append to results and display\n",
    "results = results.append(univariate_eval_df)\n",
    "results.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t--zb4h3AOeM"
   },
   "source": [
    "## Wrapper Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8eoK3htz1Jc"
   },
   "source": [
    "### Recursive Feature Elimination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6PGlE5gM8ca"
   },
   "outputs": [],
   "source": [
    "def run_rfe():\n",
    "    \n",
    "    # Split train and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
    "    \n",
    "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define the model\n",
    "    model = RandomForestClassifier(criterion='entropy', random_state=47)\n",
    "    \n",
    "    # Wrap RFE around the model\n",
    "    rfe = RFE(model, 20)\n",
    "    \n",
    "    # Fit RFE\n",
    "    rfe = rfe.fit(X_train_scaled, Y_train)\n",
    "    feature_names = df.drop(\"diagnosis_int\",1 ).columns[rfe.get_support()]\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "rfe_feature_names = run_rfe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-p-YdDuOD1B"
   },
   "outputs": [],
   "source": [
    "# Calculate and check model metrics\n",
    "rfe_eval_df = evaluate_model_on_features(df[rfe_feature_names], Y)\n",
    "rfe_eval_df.index = ['RFE']\n",
    "\n",
    "# Append to results and display\n",
    "results = results.append(rfe_eval_df)\n",
    "results.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A262FCClADGK"
   },
   "source": [
    "### Feature Importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ey_Qr2a89yxL"
   },
   "outputs": [],
   "source": [
    "def feature_importances_from_tree_based_model_():\n",
    "    \n",
    "    # Split train and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
    "    \n",
    "    # Define the model to use\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    model = model.fit(X_train_scaled,Y_train)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    feat_importances.sort_values(ascending=False).plot(kind='barh')\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def select_features_from_model(model):\n",
    "    \n",
    "    model = SelectFromModel(model, prefit=True, threshold=0.013)\n",
    "    feature_idx = model.get_support()\n",
    "    feature_names = df.drop(\"diagnosis_int\",1 ).columns[feature_idx]\n",
    "        \n",
    "    return feature_names\n",
    "\n",
    "model = feature_importances_from_tree_based_model_()\n",
    "feature_imp_feature_names = select_features_from_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqNy8BXD4mOk"
   },
   "outputs": [],
   "source": [
    "# Calculate and check model metrics\n",
    "feat_imp_eval_df = evaluate_model_on_features(df[feature_imp_feature_names], Y)\n",
    "feat_imp_eval_df.index = ['Feature Importance']\n",
    "\n",
    "# Append to results and display\n",
    "results = results.append(feat_imp_eval_df)\n",
    "results.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsERMA-IFQ_z"
   },
   "source": [
    "### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9J_gU9kQSPF"
   },
   "outputs": [],
   "source": [
    "def run_l1_regularization():\n",
    "    \n",
    "    # Split train and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
    "    \n",
    "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Select L1 regulated features from LinearSVC output \n",
    "    selection = SelectFromModel(LinearSVC(C=1, penalty='l1', dual=False))\n",
    "    selection.fit(X_train_scaled, Y_train)\n",
    "\n",
    "    feature_names = df.drop(\"diagnosis_int\",1 ).columns[(selection.get_support())]\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "l1reg_feature_names = run_l1_regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8B3PtRAN2_L"
   },
   "outputs": [],
   "source": [
    "# Calculate and check model metrics\n",
    "l1reg_eval_df = evaluate_model_on_features(df[l1reg_feature_names], Y)\n",
    "l1reg_eval_df.index = ['L1 Reg']\n",
    "\n",
    "# Append to results and display\n",
    "results = results.append(l1reg_eval_df)\n",
    "results.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these results and also your domain knowledge, you can decide which set of features to use to train on the entire dataset. If you will be basing it on the f1 score, you may narrow it down to the `Strong features`, `Subset features` and `F-test` rows because they have the highest scores. If you want to save resources, the `F-test` will be the most optimal of these 3 because it uses the least number of features (unless you did the bonus challenge and removed more from `Subset features`). On the other hand, if you find that all the resulting scores for all approaches are acceptable, then you may just go for the method with the smallest set of features."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of Feature Selection on Breast Cancer Dataset.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
